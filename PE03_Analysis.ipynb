{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707eba3f",
   "metadata": {},
   "source": [
    "#### 1. Vocabulary Size and Matrix Shape\n",
    "- When constructing document-term matrices using both BoW and TF-IDF vectorizers (with stop words removed and the token pattern set to \\b\\w+\\b), the vocabulary size was found to be 30,661 and 30,952 terms respectably. This sizable vocabulary reflects the linguistic diversity and specificity of the Reuters corpus, which contains 10,788 documents. The resulting sparse matris had shapes of (10788, 30661) and (10788, 30952) respectably. This means each document is represented as 30,661-30,952 dimensional vectors, where most entries are zero. This is typical of real-world text corpora. Documents tend to use only a small subset of all possible words, resulting in high-dimensional but sparse feature spaces.\n",
    "\n",
    "- Changing the token pattern to r\"\\b[a-zA-Z]{1,}\\b\", which matches alphabetic tokens of length one or more, resulted in a noticeable reduction in vocabulary size compared to the original pattern r\"\\b\\w+\\b\". Specifically, the Bag-of-Words vocabulary decreased from 30,661 to 28,881 terms, while the TF-IDF vocabulary dropped from 30,952 to 29,172 terms. When stopwords were removed, the TF-IDF vocabulary similarly shrank from 30,661 to 28,881 terms. The corresponding document-term matrices also became smaller, the Bag-of-Words matrix changed from (10,788, 30,661) to (10,788, 28,881). Although this reduction improves computational efficiency by filtering out tokens containing digits, underscores, or non-alphabetic characters, it retains nearly all meaningful alphabetic words, including single-letter tokens like “I” and “a,” which can be important in natural language understanding.\n",
    "\n",
    "- This comparison demonstrates the impact of token pattern choice on vocabulary size and data representation. Using a broader pattern such as r\"\\b\\w+\\b\" captures a wider variety of tokens, including numeric and alphanumeric ones, while r\"\\b[a-zA-Z]{1,}\\b\" restricts tokens to alphabetic words, balancing vocabulary comprehensiveness with cleaner, more linguistically relevant tokens.\n",
    "\n",
    "- The nearly identical vocabulary sizes and shapes between BoW and TF-IDF under the same settings also highlight that these vectorizers differ in term weighting, not term inclusion. BoW uses raw frequency counts, while TF-IDF adjusts term importance based on their frequency across documents.\n",
    "\n",
    "#### 2. Feature Counts Before and After Stop Word Removal\n",
    "- With stop words removed, the number of non-zero features in the test document was 24-26 out of 30,661-30,952 possible features. These features correspond to terms in the sample that were present in the Reuters training vocabulary and not excluded as stop words.\n",
    "\n",
    "- Had stop words been retained, the number of non-zero features would likely be higher. However, most of those additional features would be uninformative words such as \"the,\" \"is,\" or \"and,\" which offer little semantic value in tasks like classification or clustering. Removing these common terms effectively reduces noise and emphasizes the distinctive vocabulary that captures the main topics and themes of a document. For example, after stop word removal, only terms like “economic,” “market,” “interest,” “investors,” and “rally” remained. These are rich in meaning and directly related to the content of financial news.\n",
    "\n",
    "#### 3. Behavior of Vectorizers with Out-of-Vocabulary (OOV) Terms\n",
    "- When processing the new documents, any terms that were not present in the original training vocabulary are ignored by the vectorizer. This means these unseen words do not contribute to the document’s feature representation in either the Bag-of-Words or TF-IDF models. Since the vectorizer’s vocabulary is fixed after training, it can only map known terms to feature indices. New terms simply do not have corresponding features and are effectively excluded from the numerical representation. This can lead to a loss of potentially valuable information if the new document contains many novel or rare words not seen during training.\n",
    "\n",
    "#### 4. Patterns in Terms Identified in the Test Document\n",
    "- In the BoW representation, 22 unique terms appeared, each exactly once, resulting in equal frequency counts. These terms included domain-specific vocabulary such as “market”, “rate”, “federal”, and “investors”, as well as more descriptive or sentiment-laden terms like “strong”, “excellent”, “surge”, and “fueling.” This shows that the document is relatively short and focused, with terminology concentrated around economic news and investor sentiment.\n",
    "\n",
    "- In the TF-IDF representation, although the same 22 terms were present, their importance varied based on their rarity across the entire Reuters corpus. Words like “fueling” (0.3651), “bolstered” (0.3174), and “signals” (0.2756) had the highest TF-IDF weights, suggesting that they are relatively rare in the broader corpus and therefore carry more informational weight. Conversely, more frequently used terms such as “market” (0.1146), “today” (0.1253), and “rate” (0.1334) had lower scores, indicating their ubiquity in Reuters articles and hence lower discriminative value.\n",
    "\n",
    "- This pattern brings to light TF-IDF’s strength of distinguishing between generic financial terms and more unique, context-specific language. It effectively downweights common words, even if they are not typical stop words, and emphasizes less frequent more meaningful terms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
